# OpenClaw Mesh Infrastructure Audit Report
**Date:** 2026-02-22  
**Auditor:** System Architect Agent (cron: daily-3am-arch-audit)  
**Scope:** Full mesh of 4 nodes (Nexus 10.0.0.1, Clawd 10.0.0.2, Brutus 10.0.0.3, Plutos 10.0.0.4)

---

## Executive Summary

**Overall Health Score:** 4/10  
**Trend:** â†“ Decreasing (from 6/10 yesterday)

The mesh network remains operational, but a critical configuration omission on Clawd (missing WireGuard persistent config) threatens continuity across reboots. Security exposures persist: OpenClaw gateway token exposed, Ollama API internetâ€‘exposed, SSH inconsistent, firewall absent, and API keys in worldâ€‘readable shell rc. Resource utilization remains healthy. No meaningful improvements since yesterday; several new degradations observed.

---

## Mesh Health

### Connectivity Summary
| Target    | Packets | Loss   | Avg Latency (ms) | Max Latency (ms) |
|-----------|---------|--------|------------------|------------------|
| 10.0.0.1  | 3/3     | 0%     | 13.7             | 13.9             |
| 10.0.0.2  | 3/3     | 0%     | 0.04             | 0.05             |
| 10.0.0.3  | 3/3     | 0%     | 25.4             | 26.7             |
| 10.0.0.4  | 3/3     | 0%     | 12.6             | 12.7             |

- All nodes mutually reachable with 0% packet loss.
- Latency within acceptable ranges for a siteâ€‘toâ€‘site VPN.

### WireGuard Status

**On Clawd (10.0.0.2):**
- Interface `wg0` is **UP** with IP `10.0.0.2/24`.
- Route `10.0.0.0/24 dev wg0` is present.
- However, **/etc/wireguard/wg0.conf is missing** (wireguard directory not found).
- `systemctl status wg-quick@wg0` reports **inactive (dead)**.
- The running configuration is not persistent; a reboot would likely bring down the mesh or fail to reâ€‘establish the tunnel.
- This is a **new critical issue** compared to yesterday.

**Other nodes:** Not reâ€‘verified due to access restrictions; connectivity suggests they remain up.

---

## Services

### SSH Daemon

| Node   | Status  | Port(s) Listening | Desired Port | Bind Address |
|--------|---------|-------------------|--------------|--------------|
| Clawd  | active  | 0.0.0.0:22        | 2222         | 0.0.0.0      |
| Brutus | active  | 0.0.0.0:22        | 2222 (config mentions 2222 but daemon still on 22) | 0.0.0.0 |
| Plutos | active* | 22 (presumed)     | 2222         | 0.0.0.0      |
| Nexus  | active* | 22 (presumed)     | 2222         | 0.0.0.0      |

\* Assumed from previous audit; not reâ€‘checked.

- **No node yet listens on port 2222**, the designated management port.
- All daemons bind to `0.0.0.0` (internetâ€‘exposed) instead of the mesh IP.
- Nexus previously had `PermitRootLogin yes` and `PasswordAuthentication yes`; status unclear.

### Ollama API

| Node   | Status  | Binding     | Models Available                                 |
|--------|---------|-------------|--------------------------------------------------|
| Clawd  | up      | 0.0.0.0:11434 | `mistral:7b-instruct-v0.3-q4_K_M`              |
| Brutus | up      | 0.0.0.0:11434 | `qwen2.5-coder:3b`                              |
| Plutos | up      | 0.0.0.0:11434 | `llama3.1:8b-instruct-q4_K_M`                  |
| Nexus  | N/A     | â€“           | â€“                                                |

- All inference nodes expose unauthenticated APIs to the internet (`0.0.0.0`). Should be bound to `127.0.0.1` or at least the mesh address.

### OpenClaw Gateway

- **Clawd:** Running as user systemd service: `openclaw-gateway.service` (active, PID 1244297). Bound to `127.0.0.1:18789` (loopback), preventing remote agent access.
- **Gateway token** `OPENCLAW_GATEWAY_TOKEN=63a3931e00c32d904c464e7b1f99a64ccf5ecbec1d2cddea` is stored in the unit file with permissions `644` (worldâ€‘readable). **Critical exposure.**
- **Brutus & Plutos:** Node processes expected running (per previous audit); not verified today.

### Fail2Ban

- Active on Clawd (confirmed). Assumed active on others.

---

## Security

### Firewall

- **No host firewall detected.** `ufw` not installed; `nftables` not present.
- With services binding to `0.0.0.0` (SSH, Ollama, Grafana on Clawd:3000, Prometheus:9100, etc.), the nodes are directly reachable from the internet without filtering.

### Secrets & Permissions

- **Critical:** OpenClaw gateway token in `~/.config/systemd/user/openclaw-gateway.service` (mode 644). Any local user can read it.
- **High:** `HIBP_API_KEY` and `NVIDIA_API_KEY` exported in `/home/boss/.bashrc` (mode 644). Exposed to all local users.
- SSH keys in `~/.ssh/` have correct permissions (600).
- OpenClaw config `~/.openclaw/openclaw.json` is 600 (good).

### SSH Inconsistency

- Brutus `sshd_config` declares `Port 2222` but daemon listens on 22 (service not reloaded after edit).
- Clawd and Plutos still on default port 22.
- No node binds SSH to the mesh IP (`10.0.0.x`), exposing SSH to the entire internet.
- Nexus previously allowed `root` login with passwords; likely unchanged.

### Ollama Exposure

All inference nodes listen on `0.0.0.0:11434`; the API has no authentication. Anyone on the internet can query models, potentially for abuse or cost (if cloud models were configured, but currently free local only).

---

## Configuration Drift

| Component                     | Desired State (Baseline / Best Practice)               | Observed State                                             | Drift |
|-------------------------------|--------------------------------------------------------|------------------------------------------------------------|-------|
| **WireGuard (Clawd)**         | `/etc/wireguard/wg0.conf` present; `wg-quick@wg0` active & enabled | Config **missing**; service inactive                      | ðŸ”´ Critical |
| SSH port (all nodes)          | `2222` on every node                                  | Clawd/Plutos: `22`; Brutus: `22` (config outdated)       | ðŸ”´ |
| SSH bind address              | Mesh IP (`10.0.0.x`) only                              | All bind `0.0.0.0`                                         | ðŸ”´ |
| Sudo for `boss`               | Passwordless sudo on Clawd/Brutus/Plutos              | Likely only Nexus has it (previous audit)                 | ðŸ”´ |
| OpenClaw gateway bind         | Mesh IP (`10.0.0.x`) for remote agent access         | Loopback (`127.0.0.1`) on all nodes                       | ðŸ”´ |
| OpenClaw config permissions   | `600` (ownerâ€‘only)                                    | Clawd unit file: `644`                                    | ðŸ”´ |
| Ollama bind address           | `127.0.0.1:11434` (or at least mesh IP)              | `0.0.0.0:11434` on all inference nodes                   | ðŸ”´ |
| Ollama host config (Clawd)    | `host: 0.0.0.0:11434` or `127.0.0.1:11434`           | `host: 10.0.0.4:11434` (points to Plutos â€“ incorrect)    | ðŸ”´ |
| Firewall                      | Enabled (`ufw`/`nft`), default `deny`, mesh `allow` | Not installed                                             | ðŸ”´ |
| System config backups         | Daily backup of `/etc/ssh`, `/etc/wireguard`, etc.   | **None** â€“ only workspace Git backups exist              | ðŸ”´ |
| Cron jobs health              | All scheduled jobs `ok`                               | 3 jobs in `error` state (`macro-daily-scan`, `digest-morning-briefing`, `banker-daily-fact`) | ðŸŸ¡ |

---

## Resource Utilization

**Clawd (sampled now):**
- Memory: 15Gi total, 1.5Gi used, 14Gi available, 10Gi cache.
- CPU load (1â€¯min): 0.00
- Disk: 464G total, 31G used (7%), ample free space.
- No swap.

**Brutus & Plutos:** Previous audit reported healthy headroom; no data today due to access limits. No indication of saturation.

---

## High Availability & Single Points of Failure

| Component               | Current Redundancy | Risk Level | Notes |
|-------------------------|--------------------|------------|-------|
| OpenClaw Gateway        | 1 instance (Clawd) | HIGH       | If down, external bot coordination stops. |
| NeuroSec (security)     | 1 instance (Nexus) | HIGH       | Loss of Nexus blinds SOC; consider secondary instance. |
| WireGuard mesh          | Full mesh          | **MEDIUMâ€‘HIGH** | Clawd's missing config endangers persistence; if Clawd loses WG, mesh splits (Nexus becomes isolated hub). |
| Ollama inference        | 3 nodes (distributed) | LOW   | Single node loss reduces capacity but service continues. |
| System config backups   | **None**           | **HIGH**   | No recovery path; node rebuild painful. |
| SSH access ports        | Inconsistent       | HIGH       | Port 22 exposed everywhere; no uniform port increases lockout risk during remediation. |

---

## Cost Optimization

- **Local models only:** All Ollama instances use free, onâ€‘prem models (`mistralâ€‘7B`, `qwen2.5â€‘coderâ€‘3B`, `llama3.1â€‘8B`). âœ… No inference cloud spend.
- **API keys:** `HIBP_API_KEY` (rateâ€‘limited free tier) and `NVIDIA_API_KEY` (could enable paid cloud calls if misused). Ensure they are not used in automated scripts that might bill unexpectedly.
- **OpenRouter:** Default model `openrouter/stepfun/step-3.5-flash:free` â€“ good.

---

## Backup + Recovery

**Status: CRITICAL â€“ No automated backups of system configuration.**

- The existing `maintenance/daily_backup.sh` only commits workspace changes to Git. System directories (`/etc/ssh/`, `/etc/wireguard/`, `/etc/ollama/`, systemd units, firewall rules) are **not backed up**.
- Impact: A node failure requires full manual rebuild, losing SSH host keys, WireGuard peer keys, service configurations.
- **Required:** Daily archived backups (compressed tarballs) to `memory/backups/` with rotation (keep 30â€¯days).

---

## Top 5 Recommendations

### 1. Restore WireGuard Persistence on Clawd (CRITICAL)

**Why:** Missing `/etc/wireguard/wg0.conf` and inactive `wg-quick@wg0` risk complete mesh outage on next reboot.

**Actions:**
```bash
# Recreate config from backup or known good state.
# Obtain each peer's public key and endpoint (public IP) from existing configs (e.g., /home/boss/.wg-keys/*.conf).
sudo mkdir -p /etc/wireguard
cat > /etc/wireguard/wg0.conf <<'EOF'
[Interface]
PrivateKey = <ReplaceWithClawdPrivateKey>
Address = 10.0.0.2/24
DNS = 1.1.1.1

[Peer]
# Nexus (hub)
PublicKey = <NexusPublicKey>
AllowedIPs = 10.0.0.1/32
Endpoint = <NexusPublicIP>:51820
PersistentKeepalive = 25

[Peer]
# Brutus
PublicKey = <BrutusPublicKey>
AllowedIPs = 10.0.0.3/32
Endpoint = 87.106.6.144:51820
PersistentKeepalive = 25

[Peer]
# Plutos
PublicKey = <PlutosPublicKey>
AllowedIPs = 10.0.0.4/32
Endpoint = <PlutosPublicIP>:51820
PersistentKeepalive = 25
EOF
sudo chmod 600 /etc/wireguard/wg0.conf
sudo systemctl enable --now wg-quick@wg0
wg show  # verify peers and latest handshake
```
**Note:** If private keys are lost, rotate the entire mesh by generating new keys on all nodes and exchanging public keys.

### 2. Harden SSH Consistently Across All Nodes

**Why:** Inconsistent ports and internetâ€‘exposed SSH daemons increase attack surface and operational fragility.

**Apply to every node (Nexus, Clawd, Brutus, Plutos):**
```bash
MESH_IP=$(ip -4 addr show wg0 | grep -o '10\.0\.0\.[0-9]*')
sudo tee /etc/ssh/sshd_config <<EOF
Port 2222
ListenAddress $MESH_IP
PermitRootLogin no
PasswordAuthentication no
PubkeyAuthentication yes
AllowUsers boss
UsePAM yes
Subsystem sftp /usr/lib/openssh/sftp-server
EOF
sudo sshd -t && sudo systemctl reload sshd
# Verify
ss -tuln | grep 2222   # should show $MESH_IP:2222, not 0.0.0.0:2222
```
Test from another node: `ssh -p 2222 boss@<mesh-ip>`.

### 3. Secure OpenClaw Gateway Token

**Why:** Token readable by all local users enables full gateway compromise.

```bash
sudo mkdir -p /etc/openclaw
echo "OPENCLAW_GATEWAY_TOKEN=63a3931e00c32d904c464e7b1f99a64ccf5ecbec1d2cddea" | sudo tee /etc/openclaw/token
sudo chmod 600 /etc/openclaw/token
# Remove token from user unit file
sudo sed -i '/Environment=OPENCLAW_GATEWAY_TOKEN/d' /home/boss/.config/systemd/user/openclaw-gateway.service
echo 'EnvironmentFile=/etc/openclaw/token' | sudo tee -a /home/boss/.config/systemd/user/openclaw-gateway.service
systemctl --user daemon-reload
systemctl --user restart openclaw-gateway
# Verify token isn't leaking in process list of other users
ps eww -u boss | grep OPENCLAW_GATEWAY_TOKEN || echo "Token removed from environment"
```

### 4. Restrict Ollama API & Fix Config Drift

**Why:** Unauthenticated APIs exposed to internet; Clawd's `config.yaml` incorrectly points to Plutos.

Apply on **Clawd, Brutus, Plutos**:
```bash
# Best: bind Ollama to localhost only (if agents local to node)
sudo mkdir -p /etc/systemd/system/ollama.service.d
sudo tee /etc/systemd/system/ollama.service.d/secure.conf <<'EOF'
[Service]
Environment="OLLAMA_HOST=127.0.0.1:11434"
EOF
# Remove erroneous config on Clawd
sudo rm -f /etc/ollama/config.yaml
sudo systemctl daemon-reload
sudo systemctl restart ollama
# Verify binding
ss -tuln | grep 11434   # should show 127.0.0.1:11434 only
```
If remote mesh access is required (e.g., agents on other nodes), bind to the node's mesh IP (e.g., `10.0.0.2:11434`) instead and add a corresponding firewall rule (stepâ€¯5).

### 5. Deploy Host Firewall (After SSH Portâ€¯2222 is Live)

**Why:** No firewall â†’ all 0.0.0.0 services are internetâ€‘reachable. Must restrict to mesh CIDR.

```bash
# On every node (except perhaps Nexus if it remains a minimal hub)
sudo apt-get update && sudo apt-get install -y ufw
sudo ufw --force reset
sudo ufw default deny incoming
sudo ufw default allow outgoing
# SSH on new port 2222 from mesh only
sudo ufw allow from 10.0.0.0/24 to any port 2222 proto tcp
# OpenClaw gateway (if bound to mesh IP after stepâ€¯2)
sudo ufw allow from 10.0.0.0/24 to any port 18789 proto tcp
# Ollama only if bound to mesh IP; skip if bound to localhost
# sudo ufw allow from 10.0.0.0/24 to any port 11434 proto tcp
sudo ufw --force enable
sudo ufw status verbose
```
**Caution:** Apply only after verifying SSH on port 2222 works on all nodes; otherwise you risk permanent lockout.

### Bonus: Implement System Configuration Backups

```bash
sudo mkdir -p /home/boss/.openclaw/workspace/memory/backups
cat > /home/boss/.config/systemd/user/backup-configs.service <<'EOF'
[Unit]
Description=Backup critical system configs
After=network-online.target

[Service]
Type=oneshot
ExecStart=/bin/bash -c 'tar --exclude="/etc/ssh/ssh_host_*" -czf /home/boss/.openclaw/workspace/memory/backups/configs-$(date +%Y%m%d).tar.gz /etc/ssh /etc/wireguard /etc/ollama /etc/systemd/system /etc/ufw 2>/dev/null || true'
EOF
cat > /home/boss/.config/systemd/user/backup-configs.timer <<'EOF'
[Unit]
Description=Daily system config backup

[Timer]
OnCalendar=daily
Persistent=true

[Install]
WantedBy=timers.target
EOF
systemctl --user daemon-reload
systemctl --user enable --now backup-configs.timer
```
Rotate old backups (keep last 30 days) with a simple `find â€¦ -mtime +30 -delete` in the service script.

---

## Trend vs Previous Day

| Metric                                | 2026-02-20 | 2026-02-21 | **2026-02-22** | Trend |
|---------------------------------------|------------|------------|----------------|-------|
| Overall Health Score                  | 2/10       | 6/10       | **4/10**       | ðŸ”»    |
| WireGuard config present (Clawd)      | â€”          | yes        | **no**         | ðŸ”»    |
| SSH port 2222 deployed                | no         | no         | **no**         | âž¡ï¸    |
| OpenClaw gateway token secured        | no         | no         | **no**         | âž¡ï¸    |
| Ollama API exposure (all nodes)       | yes        | yes        | **yes**        | âž¡ï¸    |
| Firewall installed/enabled            | no         | no         | **no**         | âž¡ï¸    |
| System config backups                 | no         | no         | **no**         | âž¡ï¸    |
| Cron jobs healthy (no errors)         | 3 errors   | 3 errors   | **3 errors**   | âž¡ï¸    |
| Resource headroom                     | healthy    | healthy    | **healthy**    | âž¡ï¸    |
| Mesh connectivity (ping loss)         | 0%         | 0%         | **0%**         | âž¡ï¸    |

**Interpretation:** Health declined due to the WireGuard configuration loss on Clawd. All prior recommendations remain unaddressed, accumulating risk.

---

**Report generated:** 2026-02-22 02:30 UTC  
**Next scheduled audit:** 2026-02-23 03:00 UTC  
**Auditor notes:** Access restrictions prevented full rootâ€‘level verification on all nodes; some findings rely on previous audit data and nonâ€‘privileged probes. Recommend granting elevated privileges to the audit cron or executing as root for complete coverage.
